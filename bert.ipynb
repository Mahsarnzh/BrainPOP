{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25eead79-3956-4311-9a85-0837e2b0ab3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "max_len = 50  # Adjust as needed\n",
    "embedding_dim = 50  # Adjust as needed\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data['text'])\n",
    "X_train = pad_sequences(tokenizer.texts_to_sequences(train_data['text']), maxlen=max_len, padding='post')\n",
    "X_test = pad_sequences(tokenizer.texts_to_sequences(test_data['text']), maxlen=max_len, padding='post')\n",
    "\n",
    "# Labels\n",
    "y_train = train_data['label']\n",
    "y_test = test_data['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4440c1c2-040c-45d4-8f6e-ccec54bf550d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahsaraeisinezhad/code/apps/brainPOP/venv/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "# Load the pre-trained model for sequence classification\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6ca74e9-1235-4406-8d5c-ee36590e4bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "268/268 [==============================] - 634s 2s/step - loss: 4.4988 - accuracy: 0.7025\n",
      "Epoch 2/3\n",
      "268/268 [==============================] - 638s 2s/step - loss: 4.5083 - accuracy: 0.7044\n",
      "Epoch 3/3\n",
      "268/268 [==============================] - 604s 2s/step - loss: 4.5083 - accuracy: 0.7044\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2ababf350>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25690275-320f-460e-92c0-cd0dd91b0736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 939ms/step\n",
      "4.164810657501221\n"
     ]
    }
   ],
   "source": [
    "class Grader:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def evaluate(self, text):\n",
    "        # Preprocess the input text\n",
    "        encoded_text = self.tokenizer.texts_to_sequences([text])\n",
    "        encoded_text = pad_sequences(encoded_text, maxlen=max_len, padding='post')\n",
    "\n",
    "        # Make a prediction\n",
    "        prediction = self.model.predict(encoded_text)[0].squeeze()\n",
    "\n",
    "        return prediction.item()\n",
    "\n",
    "# Create an instance of the Grader class with the trained model\n",
    "mygrader = Grader(model, tokenizer)\n",
    "\n",
    "# Example usage\n",
    "output = mygrader.evaluate(\"He donâ€™t come here no more.\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd7f0f9-6dee-48d7-be6f-33da3bcf85ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance on the test set\n",
    "test_predictions = model.predict(test_texts)\n",
    "# Compute precision and recall using test_predictions and test_labels\n",
    "# You may need to threshold the predictions based on your specific needs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27144e5f-474c-4382-934b-6a72aed568d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
