{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "290ef17a-cc86-4441-befa-e947ca6c3419",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "214/214 [==============================] - 2s 9ms/step - loss: 0.6152 - accuracy: 0.7016 - val_loss: 0.5974 - val_accuracy: 0.7154\n",
      "Epoch 2/5\n",
      "214/214 [==============================] - 2s 9ms/step - loss: 0.6113 - accuracy: 0.7016 - val_loss: 0.5990 - val_accuracy: 0.7154\n",
      "Epoch 3/5\n",
      "214/214 [==============================] - 2s 9ms/step - loss: 0.6117 - accuracy: 0.7016 - val_loss: 0.5974 - val_accuracy: 0.7154\n",
      "Epoch 4/5\n",
      "214/214 [==============================] - 2s 9ms/step - loss: 0.6101 - accuracy: 0.7016 - val_loss: 0.5984 - val_accuracy: 0.7154\n",
      "Epoch 5/5\n",
      "214/214 [==============================] - 2s 9ms/step - loss: 0.6099 - accuracy: 0.7016 - val_loss: 0.5973 - val_accuracy: 0.7154\n",
      "17/17 [==============================] - 0s 2ms/step\n",
      "Precision: 0.6925996204933587, Recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Step 1: Load and preprocess the dataset\n",
    "# Assuming you have a CSV file with two columns: 'text' and 'label'\n",
    "# 'text' contains the English sentences, and 'label' contains binary values (0 or 1) for correctness.\n",
    "# You should replace 'your_dataset.csv' with the actual path to your dataset file.\n",
    "\n",
    "# Load dataset\n",
    "dataset = pd.read_csv('train.csv')\n",
    "dataset = dataset.reset_index(drop=True)\n",
    "\n",
    "# Tokenize and pad the sequences\n",
    "max_len = 50  # Set a suitable maximum length for your sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(dataset['text'])\n",
    "sequences = tokenizer.texts_to_sequences(dataset['text'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
    "# print(\"X_train shape:\", X_train.shape)\n",
    "# print(\"y_train shape:\", y_train.shape)\n",
    "# print(\"X_test shape:\", X_test.shape)\n",
    "# print(\"y_test shape:\", y_test.shape)\n",
    "labels = dataset['label']\n",
    "\n",
    "# Step 2: Develop a suitable neural net using TensorFlow\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=32, input_length=max_len),\n",
    "    tf.keras.layers.LSTM(64),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 3: Train the neural net on the training set\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
    "X_train_np = np.array(X_train)\n",
    "y_train_np = np.array(y_train)\n",
    "X_test_np = np.array(X_test)\n",
    "y_test_np = np.array(y_test)\n",
    "\n",
    "# #####\n",
    "model.fit(X_train_np, y_train_np, epochs=5, batch_size=32, validation_data=(X_test_np, y_test_np))\n",
    "\n",
    "# Step 4: Create a Python class Grader\n",
    "class Grader:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def evaluate(self, sentence):\n",
    "        sequence = tokenizer.texts_to_sequences([sentence])\n",
    "        padded_sequence = pad_sequences(sequence, maxlen=max_len, padding='post')\n",
    "        likelihood = self.model.predict(padded_sequence)[0][0]\n",
    "        return likelihood\n",
    "\n",
    "# Create an instance of Grader\n",
    "mygrader = Grader(model)\n",
    "\n",
    "# Step 5: Evaluate the performance on the provided test set\n",
    "# Assuming you have a separate CSV file for the test set\n",
    "test_data = pd.read_csv('test.csv')\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data['text'])\n",
    "padded_test_sequences = pad_sequences(test_sequences, maxlen=max_len, padding='post')\n",
    "predictions = (mygrader.model.predict(padded_test_sequences) > 0.1).astype('int32')\n",
    "\n",
    "precision = precision_score(test_data['label'], predictions)\n",
    "recall = recall_score(test_data['label'], predictions)\n",
    "\n",
    "print(f'Precision: {precision}, Recall: {recall}')\n",
    "\n",
    "# Step 6: Discuss other ideas for improving the model\n",
    "# Depending on the performance, you can explore more advanced architectures, hyperparameter tuning, and data augmentation techniques.\n",
    "# Additionally, you might consider using pre-trained word embeddings, such as GloVe or Word2Vec, for better representation of words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56deb3c-770a-414e-b353-ee0703f38440",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb1a887-5d1e-4c34-8f0f-e4ddd96f7b57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e6803e37-b96f-4b5e-b8e3-fe3f2c0d14cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "214/214 [==============================] - 2s 9ms/step - loss: 0.6153 - accuracy: 0.7001 - val_loss: 0.5975 - val_accuracy: 0.7154\n",
      "Epoch 2/5\n",
      "214/214 [==============================] - 2s 9ms/step - loss: 0.6111 - accuracy: 0.7016 - val_loss: 0.6060 - val_accuracy: 0.7154\n",
      "Epoch 3/5\n",
      "214/214 [==============================] - 2s 9ms/step - loss: 0.6104 - accuracy: 0.7016 - val_loss: 0.5977 - val_accuracy: 0.7154\n",
      "Epoch 4/5\n",
      "214/214 [==============================] - 2s 9ms/step - loss: 0.6099 - accuracy: 0.7001 - val_loss: 0.6036 - val_accuracy: 0.7154\n",
      "Epoch 5/5\n",
      "214/214 [==============================] - 2s 9ms/step - loss: 0.6104 - accuracy: 0.7016 - val_loss: 0.5984 - val_accuracy: 0.7154\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "Likelihood that the sentence is grammatically correct: 0.6935041546821594\n",
      "17/17 [==============================] - 0s 2ms/step\n",
      "Precision: 0.6925996204933587, Recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "class Grader:\n",
    "    def __init__(self, model, tokenizer, max_len):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def evaluate(self, sentence):\n",
    "        sequence = self.tokenizer.texts_to_sequences([sentence])\n",
    "        padded_sequence = pad_sequences(sequence, maxlen=self.max_len, padding='post')\n",
    "        likelihood = self.model.predict(padded_sequence)[0][0]\n",
    "        return likelihood\n",
    "\n",
    "# Step 1: Load and preprocess the dataset\n",
    "dataset = pd.read_csv('train.csv')\n",
    "# dataset = dataset.reset_index(drop=True)\n",
    "\n",
    "max_len = 50  \n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(dataset['text'])\n",
    "sequences = tokenizer.texts_to_sequences(dataset['text'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
    "labels = dataset['label']\n",
    "\n",
    "# Step 2: Develop a suitable neural net using TensorFlow\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=32, input_length=max_len),\n",
    "    tf.keras.layers.LSTM(64),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 3: Train the neural net on the training set\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Step 3: Train the neural net on the training set\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
    "X_train_np = np.array(X_train)\n",
    "y_train_np = np.array(y_train)\n",
    "X_test_np = np.array(X_test)\n",
    "y_test_np = np.array(y_test)\n",
    "\n",
    "# #####\n",
    "model.fit(X_train_np, y_train_np, epochs=5, batch_size=32, validation_data=(X_test_np, y_test_np))\n",
    "\n",
    "\n",
    "\n",
    "# Step 4: Create an instance of Grader\n",
    "mygrader = Grader(model, tokenizer, max_len)\n",
    "\n",
    "# Step 5: Evaluate a sample sentence\n",
    "likelihood = mygrader.evaluate(\"come here no more He don't anyways you stupid bitch.\")\n",
    "print(f'Likelihood that the sentence is grammatically correct: {likelihood}')\n",
    "\n",
    "# Step 6: Evaluate the performance on the provided test set\n",
    "test_data = pd.read_csv('test.csv')\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data['text'])\n",
    "padded_test_sequences = pad_sequences(test_sequences, maxlen=max_len, padding='post')\n",
    "predictions = (model.predict(padded_test_sequences) > 0.5).astype('int32')\n",
    "\n",
    "precision = precision_score(test_data['label'], predictions)\n",
    "recall = recall_score(test_data['label'], predictions)\n",
    "\n",
    "print(f'Precision: {precision}, Recall: {recall}')\n",
    "\n",
    "# Step 7: Discuss other ideas for improving the model\n",
    "# Depending on the performance, you can explore more advanced architectures, hyperparameter tuning, and data augmentation techniques.\n",
    "# Additionally, you might consider using pre-trained word embeddings, such as GloVe or Word2Vec, for better representation of words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "defa4824-f07f-438f-927a-e35ef9a9288f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]], dtype=int32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9f494a39-a00a-4ac2-a78f-6c8e0aefa9f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The sailors rode the breeze clear of the rocks.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The weights made the rope stretch over the pul...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The mechanical doll wriggled itself loose.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you had eaten more, you would want less.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>As you eat the most, you want the least.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>I would like to could swim</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>I kicked myself</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>The bookcase ran</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>I shaved myself.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>Anson became a muscle bound.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>527 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label\n",
       "0      The sailors rode the breeze clear of the rocks.      1\n",
       "1    The weights made the rope stretch over the pul...      1\n",
       "2           The mechanical doll wriggled itself loose.      1\n",
       "3          If you had eaten more, you would want less.      1\n",
       "4             As you eat the most, you want the least.      0\n",
       "..                                                 ...    ...\n",
       "522                         I would like to could swim      0\n",
       "523                                    I kicked myself      1\n",
       "524                                   The bookcase ran      0\n",
       "525                                   I shaved myself.      1\n",
       "526                       Anson became a muscle bound.      0\n",
       "\n",
       "[527 rows x 2 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv('test.csv')\n",
    "# dataset = dataset.reset_index(drop=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4832eeb-e5bc-4a99-8afb-498f26fa1ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8a28876d-9d25-4788-aa44-3482c99ab325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text,label\n",
      "The sailors rode the breeze clear of the rocks.,1\n",
      "The weights made the rope stretch over the pulley.,1\n",
      "The mechanical doll wriggled itself loose.,1\n",
      "\"If you had eaten more, you would want less.\",1\n",
      "\"As you eat the most, you want the least.\",0\n",
      "\"The more you would want, the less you would eat.\",0\n",
      "\"I demand that the more John eat, the more he pays.\",0\n",
      "\"Mary listens to the Grateful Dead, she gets depressed.\",1\n",
      "\"The angrier Mary got, the more she looked at pictures.\",1\n"
     ]
    }
   ],
   "source": [
    "!head test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e814065-7a64-41fb-891a-d4a129c5e96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahsaraeisinezhad/code/apps/brainPOP/venv/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/mahsaraeisinezhad/code/apps/brainPOP/venv/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/mahsaraeisinezhad/code/apps/brainPOP/venv/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFGPT2Model.\n",
      "\n",
      "All the weights of TFGPT2Model were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 99s 5s/step - loss: 0.6567 - accuracy: 0.6453 - val_loss: 0.6675 - val_accuracy: 0.5688\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2c7ebab10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import TFBertModel, TFGPT2Model, BertTokenizer, GPT2Tokenizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "import json\n",
    "\n",
    "\n",
    "# Step 1: Load and preprocess the dataset\n",
    "dataset = pd.read_csv('train.csv')\n",
    "# dataset = dataset.reset_index(drop=True)\n",
    "dataset = dataset.head(800)\n",
    "\n",
    "\n",
    "max_len = 50  \n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(dataset['text'])\n",
    "sequences = tokenizer.texts_to_sequences(dataset['text'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
    "labels = dataset['label']\n",
    "\n",
    "# Step 2: Load BERT and GPT-2 models\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased', trainable=True)\n",
    "gpt2_model = TFGPT2Model.from_pretrained('gpt2', trainable=True)  # Set trainable=True for GPT-2\n",
    "\n",
    "# Step 3: Define the neural net using BERT and GPT-2 models\n",
    "input_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32)\n",
    "bert_output = bert_model(input_ids)[1]  # Using [1] to get pooled output\n",
    "gpt2_output = gpt2_model(input_ids)[0]  # GPT-2 output\n",
    "\n",
    "# Repeat BERT output for each time step in the sequence\n",
    "bert_output_expanded = tf.keras.layers.RepeatVector(max_len)(bert_output)\n",
    "concatenated_output = tf.keras.layers.Concatenate(axis=-1)([bert_output_expanded, gpt2_output])\n",
    "\n",
    "# Reshape the concatenated output\n",
    "reshaped_output = tf.keras.layers.Reshape((max_len, -1))(concatenated_output)\n",
    "\n",
    "# Additional layers\n",
    "lstm_layer = tf.keras.layers.LSTM(64)(reshaped_output)  \n",
    "output_layer = tf.keras.layers.Dense(1, activation='sigmoid')(lstm_layer)\n",
    "\n",
    "model = tf.keras.Model(inputs=input_ids, outputs=output_layer)\n",
    "\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=1e-5) \n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 4: Train the neural net on the training set\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_np = np.array(X_train)\n",
    "y_train_np = np.array(y_train)\n",
    "X_test_np = np.array(X_test)\n",
    "y_test_np = np.array(y_test)\n",
    "\n",
    "# Reshape labels to match the output shape\n",
    "y_train_np_categorical = y_train_np[:, np.newaxis]  # y_train_np is 1D\n",
    "y_test_np_categorical = y_test_np[:, np.newaxis]  # y_test_np is 1D\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "model.fit(X_train_np, y_train_np_categorical, epochs=1, batch_size=32, validation_data=(X_test_np, y_test_np_categorical), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c53318e-645a-4bfa-a486-45a580263ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a006968-0090-4598-ae5b-0c06b9cf34b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Grader:\n",
    "#     def __init__(self, model, tokenizer, max_len):\n",
    "#         self.model = model\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.max_len = max_len\n",
    "\n",
    "#     def evaluate(self, sentence):\n",
    "#         # Tokenize and pad the input sentence\n",
    "#         sequence = self.tokenizer.texts_to_sequences([sentence])\n",
    "#         padded_sequence = pad_sequences(sequence, maxlen=self.max_len, padding='post')\n",
    "        \n",
    "#         # Predict using the model\n",
    "#         likelihood = self.model.predict(padded_sequence)[0][0]\n",
    "#         return likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bb79d4a2-2eb3-4fb2-9ada-fa5da6705677",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grader:\n",
    "    def __init__(self, model, tokenizer, max_len):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def evaluate(self, sentence, threshold=0.5):\n",
    "        # Tokenize and pad the input sentence\n",
    "        sequence = self.tokenizer.texts_to_sequences([sentence])\n",
    "        padded_sequence = pad_sequences(sequence, maxlen=self.max_len, padding='post')\n",
    "        \n",
    "        # Predict using the model\n",
    "        likelihood = self.model.predict(padded_sequence)[0][0]\n",
    "\n",
    "        # Scale the output value between 0 and 1\n",
    "        min_value = np.min(likelihood)\n",
    "        max_value = np.max(likelihood)\n",
    "        \n",
    "        # Check if the range is zero\n",
    "        if max_value - min_value == 0:\n",
    "            scaled_value = 0.5  # Set a default value or handle as needed\n",
    "        else:\n",
    "            scaled_value = (likelihood - min_value) / (max_value - min_value)\n",
    "\n",
    "        # # Convert to binary classification\n",
    "        # binary_prediction = 1 if scaled_value > threshold else 0\n",
    "\n",
    "        return likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ba16b72d-e8d0-4ef3-b053-6980fb1b0d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Create an instance of Grader\n",
    "mygrader = Grader(model, tokenizer, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "85369985-3c3d-4f07-998a-e13a082efc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 89ms/step\n",
      "Likelihood that the sentence is grammatically correct: 0.70827716588974\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Evaluate a sample sentence\n",
    "likelihood = mygrader.evaluate(\"she swim do she\")\n",
    "print(f'Likelihood that the sentence is grammatically correct: {likelihood}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "af02f928-5947-4cfd-8fc6-f76ef94b3673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"fine_tuned_model\")  # Replace with the desired path\n",
    "# model.save(\"fine_tuned_model.h5\")  # Save as HDF5\n",
    "\n",
    "# # Save the tokenizer configuration\n",
    "# tokenizer_config = tokenizer.to_json()\n",
    "# with open('tokenizer_config.json', 'w', encoding='utf-8') as f:\n",
    "#     f.write(json.dumps(tokenizer_config, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "00a821fa-6aa7-47f7-8eca-4e687a08a7ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 22s 1s/step\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Evaluate the performance on the provided test set\n",
    "test_data = pd.read_csv('test.csv')\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data['text'])\n",
    "padded_test_sequences = pad_sequences(test_sequences, maxlen=max_len, padding='post')\n",
    "\n",
    "predictions = model.predict(padded_test_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4ff4ca13-109b-4f37-8092-852c99c08f83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6694774 ],\n",
       "       [0.63453555],\n",
       "       [0.5127208 ],\n",
       "       [0.63622004],\n",
       "       [0.6843329 ],\n",
       "       [0.63132036],\n",
       "       [0.4853956 ],\n",
       "       [0.41443786],\n",
       "       [0.70929575],\n",
       "       [0.6081032 ],\n",
       "       [0.63056993],\n",
       "       [0.54238784],\n",
       "       [0.6147578 ],\n",
       "       [0.69747555],\n",
       "       [0.61622953],\n",
       "       [0.49770805],\n",
       "       [0.5036871 ],\n",
       "       [0.5813705 ],\n",
       "       [0.4958618 ],\n",
       "       [0.4916681 ],\n",
       "       [0.67539823],\n",
       "       [0.6843574 ],\n",
       "       [0.56115746],\n",
       "       [0.54830366],\n",
       "       [0.5768231 ],\n",
       "       [0.6539475 ],\n",
       "       [0.71196574],\n",
       "       [0.5291939 ],\n",
       "       [0.48834005],\n",
       "       [0.47858652],\n",
       "       [0.4965896 ],\n",
       "       [0.4732234 ],\n",
       "       [0.70041144],\n",
       "       [0.58699375],\n",
       "       [0.57793266],\n",
       "       [0.67583215],\n",
       "       [0.7415329 ],\n",
       "       [0.48808947],\n",
       "       [0.55854195],\n",
       "       [0.49009028],\n",
       "       [0.48739198],\n",
       "       [0.4674394 ],\n",
       "       [0.48718944],\n",
       "       [0.57253456],\n",
       "       [0.48574626],\n",
       "       [0.4985756 ],\n",
       "       [0.48875254],\n",
       "       [0.48855662],\n",
       "       [0.7280116 ],\n",
       "       [0.55305165],\n",
       "       [0.49687177],\n",
       "       [0.52121526],\n",
       "       [0.5197648 ],\n",
       "       [0.44808024],\n",
       "       [0.65709084],\n",
       "       [0.76185447],\n",
       "       [0.5873804 ],\n",
       "       [0.47931594],\n",
       "       [0.45562246],\n",
       "       [0.5383611 ],\n",
       "       [0.59697926],\n",
       "       [0.6128968 ],\n",
       "       [0.52657294],\n",
       "       [0.56771076],\n",
       "       [0.559518  ],\n",
       "       [0.719072  ],\n",
       "       [0.6273466 ],\n",
       "       [0.66414165],\n",
       "       [0.46683714],\n",
       "       [0.48084003],\n",
       "       [0.51314783],\n",
       "       [0.50125504],\n",
       "       [0.52504647],\n",
       "       [0.48465052],\n",
       "       [0.499528  ],\n",
       "       [0.48367366],\n",
       "       [0.49052903],\n",
       "       [0.5366759 ],\n",
       "       [0.5429254 ],\n",
       "       [0.48380706],\n",
       "       [0.74672276],\n",
       "       [0.74477524],\n",
       "       [0.5554007 ],\n",
       "       [0.5512544 ],\n",
       "       [0.48209155],\n",
       "       [0.5207456 ],\n",
       "       [0.73074883],\n",
       "       [0.73386955],\n",
       "       [0.49463916],\n",
       "       [0.51430017],\n",
       "       [0.6076717 ],\n",
       "       [0.7464923 ],\n",
       "       [0.51926553],\n",
       "       [0.44946566],\n",
       "       [0.72539616],\n",
       "       [0.70028865],\n",
       "       [0.74122024],\n",
       "       [0.72731125],\n",
       "       [0.65637165],\n",
       "       [0.5074911 ],\n",
       "       [0.5621879 ],\n",
       "       [0.6757439 ],\n",
       "       [0.4779519 ],\n",
       "       [0.46430713],\n",
       "       [0.50441194],\n",
       "       [0.71308714],\n",
       "       [0.5337958 ],\n",
       "       [0.49528438],\n",
       "       [0.5328439 ],\n",
       "       [0.49272236],\n",
       "       [0.6280285 ],\n",
       "       [0.5197758 ],\n",
       "       [0.50173104],\n",
       "       [0.6843727 ],\n",
       "       [0.76691073],\n",
       "       [0.5223253 ],\n",
       "       [0.6123692 ],\n",
       "       [0.57230663],\n",
       "       [0.6789621 ],\n",
       "       [0.58652866],\n",
       "       [0.49408585],\n",
       "       [0.6053787 ],\n",
       "       [0.54076487],\n",
       "       [0.49106342],\n",
       "       [0.5421374 ],\n",
       "       [0.5729867 ],\n",
       "       [0.5963156 ],\n",
       "       [0.74228895],\n",
       "       [0.6611094 ],\n",
       "       [0.7197291 ],\n",
       "       [0.73098874],\n",
       "       [0.7069317 ],\n",
       "       [0.47349638],\n",
       "       [0.5424056 ],\n",
       "       [0.5030971 ],\n",
       "       [0.576617  ],\n",
       "       [0.6007432 ],\n",
       "       [0.6190129 ],\n",
       "       [0.64087117],\n",
       "       [0.64926547],\n",
       "       [0.54586416],\n",
       "       [0.63098454],\n",
       "       [0.585039  ],\n",
       "       [0.73289573],\n",
       "       [0.4855396 ],\n",
       "       [0.48711976],\n",
       "       [0.73706675],\n",
       "       [0.6350243 ],\n",
       "       [0.60650045],\n",
       "       [0.6396005 ],\n",
       "       [0.71733755],\n",
       "       [0.6434474 ],\n",
       "       [0.760272  ],\n",
       "       [0.6218695 ],\n",
       "       [0.61578554],\n",
       "       [0.7172605 ],\n",
       "       [0.46707708],\n",
       "       [0.7389476 ],\n",
       "       [0.73894763],\n",
       "       [0.72796905],\n",
       "       [0.49427256],\n",
       "       [0.49487296],\n",
       "       [0.5147713 ],\n",
       "       [0.6102907 ],\n",
       "       [0.5128605 ],\n",
       "       [0.7565352 ],\n",
       "       [0.73180646],\n",
       "       [0.5935354 ],\n",
       "       [0.59353554],\n",
       "       [0.5935354 ],\n",
       "       [0.61920226],\n",
       "       [0.5295544 ],\n",
       "       [0.7438147 ],\n",
       "       [0.5044413 ],\n",
       "       [0.56534547],\n",
       "       [0.7389476 ],\n",
       "       [0.47331473],\n",
       "       [0.7201934 ],\n",
       "       [0.61826134],\n",
       "       [0.68014854],\n",
       "       [0.73253137],\n",
       "       [0.74794877],\n",
       "       [0.5682634 ],\n",
       "       [0.7404412 ],\n",
       "       [0.5772329 ],\n",
       "       [0.5973325 ],\n",
       "       [0.58106834],\n",
       "       [0.70250267],\n",
       "       [0.68926024],\n",
       "       [0.6106134 ],\n",
       "       [0.57274073],\n",
       "       [0.7317322 ],\n",
       "       [0.49654016],\n",
       "       [0.7481656 ],\n",
       "       [0.60789305],\n",
       "       [0.5880195 ],\n",
       "       [0.64734185],\n",
       "       [0.6887966 ],\n",
       "       [0.7205331 ],\n",
       "       [0.73935956],\n",
       "       [0.7360777 ],\n",
       "       [0.7445065 ],\n",
       "       [0.6798143 ],\n",
       "       [0.73227495],\n",
       "       [0.5015795 ],\n",
       "       [0.7382479 ],\n",
       "       [0.6297257 ],\n",
       "       [0.5215188 ],\n",
       "       [0.63901913],\n",
       "       [0.50174826],\n",
       "       [0.73253137],\n",
       "       [0.7231041 ],\n",
       "       [0.48264295],\n",
       "       [0.62089753],\n",
       "       [0.47121525],\n",
       "       [0.73180646],\n",
       "       [0.7393598 ],\n",
       "       [0.66406447],\n",
       "       [0.74162096],\n",
       "       [0.47538042],\n",
       "       [0.50522035],\n",
       "       [0.4868433 ],\n",
       "       [0.6352492 ],\n",
       "       [0.73258865],\n",
       "       [0.45990917],\n",
       "       [0.7118249 ],\n",
       "       [0.48745784],\n",
       "       [0.7174377 ],\n",
       "       [0.75619876],\n",
       "       [0.6268992 ],\n",
       "       [0.65488404],\n",
       "       [0.49962014],\n",
       "       [0.7426615 ],\n",
       "       [0.7558086 ],\n",
       "       [0.46454588],\n",
       "       [0.48955333],\n",
       "       [0.7096543 ],\n",
       "       [0.70157135],\n",
       "       [0.71716684],\n",
       "       [0.7445065 ],\n",
       "       [0.49133453],\n",
       "       [0.47132435],\n",
       "       [0.48330697],\n",
       "       [0.7335103 ],\n",
       "       [0.7358835 ],\n",
       "       [0.62154424],\n",
       "       [0.75680876],\n",
       "       [0.6791443 ],\n",
       "       [0.75169826],\n",
       "       [0.7397331 ],\n",
       "       [0.7230767 ],\n",
       "       [0.63649815],\n",
       "       [0.5938802 ],\n",
       "       [0.71075165],\n",
       "       [0.4832745 ],\n",
       "       [0.5856106 ],\n",
       "       [0.59146315],\n",
       "       [0.51590425],\n",
       "       [0.73862004],\n",
       "       [0.6074125 ],\n",
       "       [0.6838392 ],\n",
       "       [0.45946613],\n",
       "       [0.48913383],\n",
       "       [0.61825097],\n",
       "       [0.4768447 ],\n",
       "       [0.44714832],\n",
       "       [0.6649959 ],\n",
       "       [0.6969364 ],\n",
       "       [0.6150279 ],\n",
       "       [0.6363983 ],\n",
       "       [0.7232495 ],\n",
       "       [0.7299093 ],\n",
       "       [0.7308554 ],\n",
       "       [0.7213053 ],\n",
       "       [0.5021353 ],\n",
       "       [0.6077371 ],\n",
       "       [0.46828675],\n",
       "       [0.72383446],\n",
       "       [0.7178215 ],\n",
       "       [0.75260735],\n",
       "       [0.70686156],\n",
       "       [0.5287066 ],\n",
       "       [0.6740394 ],\n",
       "       [0.7455423 ],\n",
       "       [0.71663755],\n",
       "       [0.7388898 ],\n",
       "       [0.48547643],\n",
       "       [0.46562484],\n",
       "       [0.52348006],\n",
       "       [0.520639  ],\n",
       "       [0.49498627],\n",
       "       [0.46070993],\n",
       "       [0.53025997],\n",
       "       [0.48102388],\n",
       "       [0.58799624],\n",
       "       [0.5513285 ],\n",
       "       [0.64072883],\n",
       "       [0.5855932 ],\n",
       "       [0.72552097],\n",
       "       [0.4888584 ],\n",
       "       [0.5427449 ],\n",
       "       [0.48053443],\n",
       "       [0.6411264 ],\n",
       "       [0.6241745 ],\n",
       "       [0.59214044],\n",
       "       [0.5905217 ],\n",
       "       [0.5805749 ],\n",
       "       [0.48978925],\n",
       "       [0.7056064 ],\n",
       "       [0.6964055 ],\n",
       "       [0.7498022 ],\n",
       "       [0.6413885 ],\n",
       "       [0.48724997],\n",
       "       [0.5736909 ],\n",
       "       [0.71134573],\n",
       "       [0.67245084],\n",
       "       [0.60320216],\n",
       "       [0.70508265],\n",
       "       [0.7027273 ],\n",
       "       [0.7506741 ],\n",
       "       [0.47873828],\n",
       "       [0.6495215 ],\n",
       "       [0.6715122 ],\n",
       "       [0.505408  ],\n",
       "       [0.5010236 ],\n",
       "       [0.5126842 ],\n",
       "       [0.49028477],\n",
       "       [0.53826535],\n",
       "       [0.615972  ],\n",
       "       [0.5593128 ],\n",
       "       [0.54510504],\n",
       "       [0.5562993 ],\n",
       "       [0.48301673],\n",
       "       [0.7193517 ],\n",
       "       [0.7429731 ],\n",
       "       [0.52959174],\n",
       "       [0.58846855],\n",
       "       [0.50647247],\n",
       "       [0.5969963 ],\n",
       "       [0.48974365],\n",
       "       [0.70325434],\n",
       "       [0.73086244],\n",
       "       [0.50346243],\n",
       "       [0.72259134],\n",
       "       [0.46184146],\n",
       "       [0.6568026 ],\n",
       "       [0.4875557 ],\n",
       "       [0.52277845],\n",
       "       [0.6068208 ],\n",
       "       [0.53092664],\n",
       "       [0.4710488 ],\n",
       "       [0.5395963 ],\n",
       "       [0.7324043 ],\n",
       "       [0.5522618 ],\n",
       "       [0.6881195 ],\n",
       "       [0.69599664],\n",
       "       [0.63798296],\n",
       "       [0.48159683],\n",
       "       [0.464562  ],\n",
       "       [0.48802897],\n",
       "       [0.54880136],\n",
       "       [0.5393193 ],\n",
       "       [0.54823226],\n",
       "       [0.6001991 ],\n",
       "       [0.619566  ],\n",
       "       [0.7424771 ],\n",
       "       [0.72608   ],\n",
       "       [0.5563258 ],\n",
       "       [0.47661957],\n",
       "       [0.61814064],\n",
       "       [0.5626115 ],\n",
       "       [0.5510765 ],\n",
       "       [0.74730957],\n",
       "       [0.5801306 ],\n",
       "       [0.7143157 ],\n",
       "       [0.49009177],\n",
       "       [0.6287818 ],\n",
       "       [0.6724508 ],\n",
       "       [0.750577  ],\n",
       "       [0.61370325],\n",
       "       [0.7219455 ],\n",
       "       [0.4848659 ],\n",
       "       [0.5883992 ],\n",
       "       [0.49304938],\n",
       "       [0.58476436],\n",
       "       [0.6264234 ],\n",
       "       [0.5011809 ],\n",
       "       [0.4972801 ],\n",
       "       [0.5424858 ],\n",
       "       [0.7413642 ],\n",
       "       [0.70958686],\n",
       "       [0.50546837],\n",
       "       [0.50546837],\n",
       "       [0.7156033 ],\n",
       "       [0.51655287],\n",
       "       [0.49816734],\n",
       "       [0.73833233],\n",
       "       [0.50772256],\n",
       "       [0.56549007],\n",
       "       [0.4869898 ],\n",
       "       [0.4804078 ],\n",
       "       [0.70475847],\n",
       "       [0.7475327 ],\n",
       "       [0.5402125 ],\n",
       "       [0.52993816],\n",
       "       [0.60636973],\n",
       "       [0.46341065],\n",
       "       [0.6776935 ],\n",
       "       [0.550331  ],\n",
       "       [0.53750306],\n",
       "       [0.6704155 ],\n",
       "       [0.7120136 ],\n",
       "       [0.5489047 ],\n",
       "       [0.7144839 ],\n",
       "       [0.7130603 ],\n",
       "       [0.5081978 ],\n",
       "       [0.59921926],\n",
       "       [0.6464745 ],\n",
       "       [0.62962145],\n",
       "       [0.5480475 ],\n",
       "       [0.58590394],\n",
       "       [0.59402645],\n",
       "       [0.7489611 ],\n",
       "       [0.6251196 ],\n",
       "       [0.69218546],\n",
       "       [0.64017254],\n",
       "       [0.6898739 ],\n",
       "       [0.4944026 ],\n",
       "       [0.7350375 ],\n",
       "       [0.47734684],\n",
       "       [0.5430609 ],\n",
       "       [0.47499323],\n",
       "       [0.4585183 ],\n",
       "       [0.5624856 ],\n",
       "       [0.51442313],\n",
       "       [0.5720364 ],\n",
       "       [0.51978326],\n",
       "       [0.46014506],\n",
       "       [0.73979896],\n",
       "       [0.7030471 ],\n",
       "       [0.4825543 ],\n",
       "       [0.5093029 ],\n",
       "       [0.76080817],\n",
       "       [0.4729311 ],\n",
       "       [0.7138918 ],\n",
       "       [0.62422967],\n",
       "       [0.5231293 ],\n",
       "       [0.7318064 ],\n",
       "       [0.74569696],\n",
       "       [0.67258334],\n",
       "       [0.720808  ],\n",
       "       [0.7015376 ],\n",
       "       [0.45762983],\n",
       "       [0.749444  ],\n",
       "       [0.70597225],\n",
       "       [0.5247834 ],\n",
       "       [0.4780305 ],\n",
       "       [0.6069606 ],\n",
       "       [0.73170036],\n",
       "       [0.4774839 ],\n",
       "       [0.48064584],\n",
       "       [0.4846321 ],\n",
       "       [0.54897195],\n",
       "       [0.56160015],\n",
       "       [0.60685295],\n",
       "       [0.5663757 ],\n",
       "       [0.5035257 ],\n",
       "       [0.4972077 ],\n",
       "       [0.6743723 ],\n",
       "       [0.7145228 ],\n",
       "       [0.6135204 ],\n",
       "       [0.6500823 ],\n",
       "       [0.5173128 ],\n",
       "       [0.73180646],\n",
       "       [0.72668964],\n",
       "       [0.73935974],\n",
       "       [0.73258865],\n",
       "       [0.51133037],\n",
       "       [0.7331992 ],\n",
       "       [0.4628267 ],\n",
       "       [0.73258865],\n",
       "       [0.6361257 ],\n",
       "       [0.6796284 ],\n",
       "       [0.57895696],\n",
       "       [0.5620327 ],\n",
       "       [0.49669   ],\n",
       "       [0.48530093],\n",
       "       [0.50568783],\n",
       "       [0.5010531 ],\n",
       "       [0.7318065 ],\n",
       "       [0.74259204],\n",
       "       [0.59533054],\n",
       "       [0.54518926],\n",
       "       [0.49969593],\n",
       "       [0.47597122],\n",
       "       [0.48755464],\n",
       "       [0.65683097],\n",
       "       [0.507329  ],\n",
       "       [0.5215608 ],\n",
       "       [0.49182937],\n",
       "       [0.7490266 ],\n",
       "       [0.46724188],\n",
       "       [0.60425204],\n",
       "       [0.73012453],\n",
       "       [0.4813206 ],\n",
       "       [0.49863166],\n",
       "       [0.4753164 ],\n",
       "       [0.5398874 ],\n",
       "       [0.6228203 ],\n",
       "       [0.63625133],\n",
       "       [0.7323804 ],\n",
       "       [0.6685544 ],\n",
       "       [0.49978977],\n",
       "       [0.49842924],\n",
       "       [0.77101314],\n",
       "       [0.6581081 ],\n",
       "       [0.7022874 ],\n",
       "       [0.63724697],\n",
       "       [0.59769356],\n",
       "       [0.6142319 ],\n",
       "       [0.6460305 ],\n",
       "       [0.73553604],\n",
       "       [0.66295546],\n",
       "       [0.7389316 ],\n",
       "       [0.501466  ],\n",
       "       [0.73893166],\n",
       "       [0.7149824 ]], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0c78068e-79c3-4a4c-98a8-51cc5faa765c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7026022304832714, Recall: 0.5178082191780822\n"
     ]
    }
   ],
   "source": [
    "min_value = np.min(predictions)\n",
    "max_value = np.max(predictions)\n",
    "scaled_values = (predictions - min_value) / (max_value - min_value)\n",
    "\n",
    "binary_predictions = np.where(scaled_values > 0.5, 1, 0)\n",
    "y_true = test_data['label'].astype(int)\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision = precision_score(y_true, binary_predictions)\n",
    "recall = recall_score(y_true, binary_predictions)\n",
    "\n",
    "\n",
    "# precision = precision_score(test_data['label'], predictions)\n",
    "# recall = recall_score(test_data['label'], predictions)\n",
    "\n",
    "print(f'Precision: {precision}, Recall: {recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4f0161ca-7aa2-433d-a065-c14a33fd553b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6dda7dba-8177-4926-a10c-8970f1fa5a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(527,)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adde23d6-8668-403b-ba76-1e4e59a55803",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18de9708-8708-4e8c-b07e-dff98b90a12f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "dfcc63d2-9646-4361-a356-5279c87f5990",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.data.processors.glue import ColaProcessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7afcf7-403a-4fff-9062-0e210d4fd4cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ef2db3-3e44-4c1e-a2c4-b7f65e58ec14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0def8117-9c59-41be-9389-2744616669e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bbba19-8821-4c92-a75a-02f606de1525",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d49c8d-bcba-4d92-a962-1fe82ac39e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bc9137-988a-4d8f-a810-b3d5a8f27b48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f72a2c16-2849-44af-8c02-497dd27a42eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "214/214 [==============================] - 1s 4ms/step - loss: 0.6215 - accuracy: 0.6984 - val_loss: 0.5880 - val_accuracy: 0.7253\n",
      "Epoch 2/5\n",
      "214/214 [==============================] - 1s 5ms/step - loss: 0.6142 - accuracy: 0.6991 - val_loss: 0.5883 - val_accuracy: 0.7253\n",
      "Epoch 3/5\n",
      "214/214 [==============================] - 1s 4ms/step - loss: 0.6142 - accuracy: 0.6991 - val_loss: 0.5918 - val_accuracy: 0.7253\n",
      "Epoch 4/5\n",
      "214/214 [==============================] - 1s 4ms/step - loss: 0.6128 - accuracy: 0.6991 - val_loss: 0.5898 - val_accuracy: 0.7253\n",
      "Epoch 5/5\n",
      "214/214 [==============================] - 1s 4ms/step - loss: 0.6136 - accuracy: 0.6991 - val_loss: 0.5906 - val_accuracy: 0.7253\n",
      "17/17 [==============================] - 0s 946us/step - loss: 0.6170 - accuracy: 0.6926\n",
      "RNN Test Loss: 0.6170089840888977, Test Accuracy: 0.6925995945930481\n",
      "17/17 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "max_len = 50  # Adjust as needed\n",
    "embedding_dim = 50  # Adjust as needed\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data['text'])\n",
    "X_train = pad_sequences(tokenizer.texts_to_sequences(train_data['text']), maxlen=max_len, padding='post')\n",
    "X_test = pad_sequences(tokenizer.texts_to_sequences(test_data['text']), maxlen=max_len, padding='post')\n",
    "\n",
    "# Labels\n",
    "y_train = train_data['label']\n",
    "y_test = test_data['label']\n",
    "\n",
    "# RNN model\n",
    "rnn_model = Sequential()\n",
    "rnn_model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim, input_length=max_len))\n",
    "rnn_model.add(SimpleRNN(64, activation='relu'))\n",
    "rnn_model.add(Dense(64, activation='relu'))\n",
    "rnn_model.add(Dropout(0.5))\n",
    "rnn_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# optimizer = Adam(learning_rate=0.001)\n",
    "# rnn_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "rnn_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Train the model\n",
    "rnn_model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate on test set\n",
    "loss, accuracy = rnn_model.evaluate(X_test, y_test)\n",
    "print(f'RNN Test Loss: {loss}, Test Accuracy: {accuracy}')\n",
    "\n",
    "# Make predictions\n",
    "rnn_predictions = rnn_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "031c9e7e-cbfa-4e6f-bed4-fc47653ec695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence: [[34, 4468, 90, 2]]\n",
      "Padded Sequence: [[  34 4468   90    2    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0]]\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "Likelihood: 0.6912722587585449\n",
      "Likelihood that the sentence is grammatically correct: 0.6912722587585449\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def evaluate_sentence(sentence, model, tokenizer, max_len):\n",
    "    # Tokenize and pad the input sentence\n",
    "    sequence = tokenizer.texts_to_sequences([sentence])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_len, padding='post')\n",
    "\n",
    "    # Print for debugging\n",
    "    print(f'Sequence: {sequence}')\n",
    "    print(f'Padded Sequence: {padded_sequence}')\n",
    "\n",
    "    # Make prediction using the trained model\n",
    "    likelihood = model.predict(padded_sequence)[0][0]\n",
    "\n",
    "    # Print for debugging\n",
    "    print(f'Likelihood: {likelihood}')\n",
    "\n",
    "    return likelihood\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'model', 'tokenizer', and 'max_len' are already defined\n",
    "sentence_to_evaluate = \"she swims no to.\"\n",
    "result = evaluate_sentence(sentence_to_evaluate, rnn_model, tokenizer, max_len)\n",
    "\n",
    "# Print the result\n",
    "print(f'Likelihood that the sentence is grammatically correct: {result}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f39f09-850c-4a79-ba5f-3ad2e0bf496a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c650cf45-23f0-44a3-a7d0-6a5cd6b8aad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f68d00-65ce-498a-94e8-4e72fb80daad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9b5157b4-ef61-42e1-8b5d-b96c97ea75e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "214/214 [==============================] - 3s 9ms/step - loss: 0.6224 - accuracy: 0.6990 - val_loss: 0.6002 - val_accuracy: 0.7253\n",
      "Epoch 2/5\n",
      "214/214 [==============================] - 2s 9ms/step - loss: 0.7012 - accuracy: 0.6990 - val_loss: 0.5901 - val_accuracy: 0.7253\n",
      "Epoch 3/5\n",
      "214/214 [==============================] - 2s 9ms/step - loss: 0.6162 - accuracy: 0.6991 - val_loss: 0.5879 - val_accuracy: 0.7253\n",
      "Epoch 4/5\n",
      "214/214 [==============================] - 2s 9ms/step - loss: 0.6146 - accuracy: 0.6991 - val_loss: 0.5927 - val_accuracy: 0.7253\n",
      "Epoch 5/5\n",
      "214/214 [==============================] - 2s 9ms/step - loss: 0.6136 - accuracy: 0.6991 - val_loss: 0.5889 - val_accuracy: 0.7253\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.6173 - accuracy: 0.6926\n",
      "LSTM Test Loss: 0.6173422336578369, Test Accuracy: 0.6925995945930481\n",
      "17/17 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "max_len = 50  # Adjust as needed\n",
    "embedding_dim = 50  # Adjust as needed\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data['text'])\n",
    "X_train = pad_sequences(tokenizer.texts_to_sequences(train_data['text']), maxlen=max_len, padding='post')\n",
    "X_test = pad_sequences(tokenizer.texts_to_sequences(test_data['text']), maxlen=max_len, padding='post')\n",
    "\n",
    "# Labels\n",
    "y_train = train_data['label']\n",
    "y_test = test_data['label']\n",
    "\n",
    "# LSTM model\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim, input_length=max_len))\n",
    "lstm_model.add(LSTM(64, activation='relu'))\n",
    "lstm_model.add(Dense(64, activation='relu'))\n",
    "lstm_model.add(Dropout(0.5))\n",
    "lstm_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "lstm_model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate on test set\n",
    "loss, accuracy = lstm_model.evaluate(X_test, y_test)\n",
    "print(f'LSTM Test Loss: {loss}, Test Accuracy: {accuracy}')\n",
    "\n",
    "# Make predictions\n",
    "lstm_predictions = lstm_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "3968ba48-01fd-413b-84b5-b3c26aab49a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence: [[5, 200, 1978, 25, 48]]\n",
      "Padded Sequence: [[   5  200 1978   25   48    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0]]\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "Likelihood: 0.704561173915863\n",
      "Likelihood that the sentence is grammatically correct: 0.704561173915863\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def evaluate_sentence(sentence, model, tokenizer, max_len):\n",
    "    # Tokenize and pad the input sentence\n",
    "    sequence = tokenizer.texts_to_sequences([sentence])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_len, padding='post')\n",
    "\n",
    "    # Print for debugging\n",
    "    print(f'Sequence: {sequence}')\n",
    "    print(f'Padded Sequence: {padded_sequence}')\n",
    "\n",
    "    # Make prediction using the trained model\n",
    "    likelihood = model.predict(padded_sequence)[0][0]\n",
    "\n",
    "    # Print for debugging\n",
    "    print(f'Likelihood: {likelihood}')\n",
    "\n",
    "    return likelihood\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'model', 'tokenizer', and 'max_len' are already defined\n",
    "sentence_to_evaluate = \"I am swim more than dk.\"\n",
    "result = evaluate_sentence(sentence_to_evaluate, lstm_model, tokenizer, max_len)\n",
    "\n",
    "# Print the result\n",
    "print(f'Likelihood that the sentence is grammatically correct: {result}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c488b7e-e37e-467f-9b68-67087e06258b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad50f574-6639-49aa-893b-667ea2a68bea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fed1233-e196-49db-a5b9-18abdd8120a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bb39b5-df88-4749-a379-5f4e669eb3b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2ba1f3-c986-4fff-85b9-ae924e064320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1055db-e330-43ad-9bdc-975590ba1c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5d513d-ef53-4a9f-9d04-e44134f678e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae5ffd1-c3bf-47fd-9719-dc9249d5f850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285a80b3-a0e3-4c27-9221-d4ae4bdcd8f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d742e4-629b-4889-b50b-e3e38cf03d16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3adfa40-6277-4a2f-8f36-c72e77cf35da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77962b2-d31d-49a8-815f-cb0a9a734251",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66f35de-fc3a-4fc5-843d-eb7123ae90ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415fc6ff-a463-4541-a932-31966533f717",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
