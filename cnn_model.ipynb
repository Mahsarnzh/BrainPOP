{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "528fe5ba-bd0c-42b3-8850-43bd81ee2465",
   "metadata": {},
   "source": [
    "### CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0f6da93b-7d76-4750-8919-0783be46fd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.2148 - accuracy: 0.6961 - val_loss: 0.2003 - val_accuracy: 0.7253\n",
      "Epoch 2/5\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.1961 - accuracy: 0.7096 - val_loss: 0.2051 - val_accuracy: 0.7101\n",
      "Epoch 3/5\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.1458 - accuracy: 0.7920 - val_loss: 0.2213 - val_accuracy: 0.6540\n",
      "Epoch 4/5\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.0956 - accuracy: 0.8747 - val_loss: 0.2345 - val_accuracy: 0.6721\n",
      "Epoch 5/5\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.0651 - accuracy: 0.9162 - val_loss: 0.2560 - val_accuracy: 0.6587\n",
      "17/17 [==============================] - 0s 939us/step - loss: 0.2586 - accuracy: 0.6622\n",
      "Test Loss: 0.2586108446121216, Test Accuracy: 0.6622390747070312\n",
      "17/17 [==============================] - 0s 973us/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "max_len = 50  # Adjust as needed\n",
    "embedding_dim = 50  # Adjust as needed\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data['text'])\n",
    "X_train = pad_sequences(tokenizer.texts_to_sequences(train_data['text']), maxlen=max_len, padding='post')\n",
    "X_test = pad_sequences(tokenizer.texts_to_sequences(test_data['text']), maxlen=max_len, padding='post')\n",
    "\n",
    "# Labels\n",
    "y_train = train_data['label']\n",
    "y_test = test_data['label']\n",
    "\n",
    "# CNN model\n",
    "cnn_model = Sequential()\n",
    "cnn_model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim, input_length=max_len))\n",
    "cnn_model.add(Conv1D(128, 5, activation='relu'))\n",
    "cnn_model.add(GlobalMaxPooling1D())\n",
    "cnn_model.add(Dense(64, activation='relu'))\n",
    "cnn_model.add(Dropout(0.5))\n",
    "cnn_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "cnn_model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate on test set\n",
    "loss, accuracy = cnn_model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n",
    "\n",
    "# Make predictions\n",
    "predictions = cnn_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e9919228-2eb8-4ce8-8002-8dca145bcb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence: [[12, 216, 235, 90, 25]]\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Grammatically \u001b[1m INCORRECT \u001b[0m with probability:  0.7341004908084869\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def evaluate_sentence(sentence, model, tokenizer, max_len):\n",
    "    # Tokenize and pad the input sentence\n",
    "    sequence = tokenizer.texts_to_sequences([sentence])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_len, padding='post')\n",
    "\n",
    "    # Print for debugging\n",
    "    print(f'Sequence: {sequence}')\n",
    "    # print(f'Padded Sequence: {padded_sequence}')\n",
    "\n",
    "    # Make prediction using the trained model\n",
    "    likelihood = model.predict(padded_sequence)[0][0]\n",
    "\n",
    "    return likelihood\n",
    "\n",
    "# Example usage:\n",
    "sentence_to_evaluate = \"He don’t come here no more.\"\n",
    "result = evaluate_sentence(sentence_to_evaluate, cnn_model, tokenizer, max_len)\n",
    "if result<0.6:\n",
    "    print(\"Grammatically \\033[1m INCORRECT \\033[0m with probability: \",1- result)\n",
    "else:\n",
    "    print(\"Grammatically \\033[1m CORRECT \\033[0m with probability: \",result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "eb1a799b-bfdf-46ba-88ca-d04321a4a8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7251184834123223, Recall: 0.8383561643835616\n"
     ]
    }
   ],
   "source": [
    "min_value = np.min(predictions)\n",
    "max_value = np.max(predictions)\n",
    "scaled_values = (predictions - min_value) / (max_value - min_value)\n",
    "\n",
    "binary_predictions = np.where(scaled_values > 0.4, 1, 0)\n",
    "y_true = test_data['label'].astype(int)\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision = precision_score(y_true, binary_predictions)\n",
    "recall = recall_score(y_true, binary_predictions)\n",
    "\n",
    "print(f'Precision: {precision}, Recall: {recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9319740-663c-4733-a012-2ed4bb47fe60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6b60c80-6460-4ccb-81ac-d8f9c75faca1",
   "metadata": {},
   "source": [
    "### CNN model with GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5ca1f168-69da-42a5-9de7-4c5ba48c4b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "214/214 [==============================] - 2s 7ms/step - loss: 0.2274 - accuracy: 0.6943 - val_loss: 0.2014 - val_accuracy: 0.7253\n",
      "Epoch 2/5\n",
      "214/214 [==============================] - 1s 6ms/step - loss: 0.2086 - accuracy: 0.6985 - val_loss: 0.1999 - val_accuracy: 0.7253\n",
      "Epoch 3/5\n",
      "214/214 [==============================] - 1s 6ms/step - loss: 0.2051 - accuracy: 0.6981 - val_loss: 0.2054 - val_accuracy: 0.7253\n",
      "Epoch 4/5\n",
      "214/214 [==============================] - 1s 7ms/step - loss: 0.1998 - accuracy: 0.6991 - val_loss: 0.2259 - val_accuracy: 0.7253\n",
      "Epoch 5/5\n",
      "214/214 [==============================] - 1s 7ms/step - loss: 0.1951 - accuracy: 0.6985 - val_loss: 0.2105 - val_accuracy: 0.7253\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.2147 - accuracy: 0.6926\n",
      "CNN Test Loss: 0.2147371917963028, Test Accuracy: 0.6925995945930481\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "max_len = 50  # Adjust as needed\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data['text'])\n",
    "X_train = pad_sequences(tokenizer.texts_to_sequences(train_data['text']), maxlen=max_len, padding='post')\n",
    "X_test = pad_sequences(tokenizer.texts_to_sequences(test_data['text']), maxlen=max_len, padding='post')\n",
    "\n",
    "# Labels\n",
    "y_train = train_data['label']\n",
    "y_test = test_data['label']\n",
    "\n",
    "\n",
    "class Grader:\n",
    "    def __init__(self, tokenizer, model):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "\n",
    "    def evaluate(self, sentence):\n",
    "        # Tokenize and pad the input sentence using the same tokenizer and padding method used during training\n",
    "        sequence = pad_sequences(self.tokenizer.texts_to_sequences([sentence]), maxlen=max_len,  padding='post')\n",
    "\n",
    "        # Print the tokenized sequence for debugging\n",
    "        print(f'Tokenized sequence for \"{sentence}\": {sequence}')\n",
    "\n",
    "        # Make predictions\n",
    "        prediction = self.model.predict(sequence)[0][0]\n",
    "        print(self.model.predict(sequence))\n",
    "        return prediction\n",
    "\n",
    "\n",
    "# Load GloVe embeddings\n",
    "embedding_dim = 300  \n",
    "embedding_index = {}\n",
    "glove_path = 'glove.6B.300d.txt'  \n",
    "\n",
    "with open(glove_path, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = coefs\n",
    "\n",
    "# Create an embedding matrix\n",
    "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# CNN model\n",
    "cnn_model = Sequential()\n",
    "cnn_model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim, input_length=max_len, weights=[embedding_matrix], trainable=False))\n",
    "cnn_model.add(Conv1D(128, 5, activation='relu'))\n",
    "cnn_model.add(GlobalMaxPooling1D())\n",
    "cnn_model.add(Dense(64, activation='relu'))\n",
    "cnn_model.add(Dropout(0.5))\n",
    "cnn_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile and train the model\n",
    "optimizer = Adam(learning_rate=0.01)\n",
    "cnn_model.compile(optimizer=optimizer, loss='mse', metrics=['accuracy'])\n",
    "cnn_model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate on test set\n",
    "loss, accuracy = cnn_model.evaluate(X_test, y_test)\n",
    "print(f'CNN Test Loss: {loss}, Test Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "aeca2dc2-6823-4ef7-88f3-c15c269a06e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3370a657-7373-45f1-8a7f-1c6d3e8094fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sequence for \"He don’t come here no more.\": [[ 12 216 235  90  25   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "[[0.63015014]]\n",
      "The likelihood that the sentence is grammatically correct: 0.6301501393318176\n"
     ]
    }
   ],
   "source": [
    "# Create a Grader instance\n",
    "mygrader = Grader(tokenizer, cnn_model)\n",
    "\n",
    "# Test the Grader\n",
    "sentence = \"He don’t come here no more.\"\n",
    "result = mygrader.evaluate(sentence)\n",
    "print(f\"The likelihood that the sentence is grammatically correct: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dbfee1ef-6010-44e7-8734-34e92476f5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sequence for \"I comes.\": [[   5 1210    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0]]\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "[[0.69435024]]\n",
      "Tokenized sequence for \"This is a different sentence.\": [[  28    6    3  581 2366    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0]]\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "[[0.71503776]]\n",
      "Tokenized sequence for \"Another example sentence.\": [[1840 2441 2366    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0]]\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "[[0.62563956]]\n",
      "The likelihood that the first sentence is grammatically correct: 0.6943502426147461\n",
      "The likelihood that the second sentence is grammatically correct: 0.7150377631187439\n",
      "The likelihood that the third sentence is grammatically correct: 0.6256395578384399\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Test the Grader\n",
    "sentence_1 = \"I comes.\"\n",
    "sentence_2 = \"This is a different sentence.\"\n",
    "sentence_3 = \"Another example sentence.\"\n",
    "\n",
    "result_1 = mygrader.evaluate(sentence_1)\n",
    "result_2 = mygrader.evaluate(sentence_2)\n",
    "result_3 = mygrader.evaluate(sentence_3)\n",
    "\n",
    "print(f\"The likelihood that the first sentence is grammatically correct: {result_1}\")\n",
    "print(f\"The likelihood that the second sentence is grammatically correct: {result_2}\")\n",
    "print(f\"The likelihood that the third sentence is grammatically correct: {result_3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e5ac9d-d604-4105-a684-4fbdbf3edf73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
