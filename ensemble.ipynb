{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "631f4d0b-be9c-48b6-9cc5-897bd6163b03",
   "metadata": {},
   "source": [
    "### Ensemble Learning: Stacking Method with CNN, RNN, and RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aef05bf3-33ed-442e-80ae-1e428e595c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.2131 - accuracy: 0.6996 - val_loss: 0.1994 - val_accuracy: 0.7253\n",
      "Epoch 2/5\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.1927 - accuracy: 0.7064 - val_loss: 0.2031 - val_accuracy: 0.7236\n",
      "Epoch 3/5\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.1467 - accuracy: 0.7885 - val_loss: 0.2126 - val_accuracy: 0.6926\n",
      "Epoch 4/5\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.1021 - accuracy: 0.8632 - val_loss: 0.2470 - val_accuracy: 0.6160\n",
      "Epoch 5/5\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.0708 - accuracy: 0.9117 - val_loss: 0.2551 - val_accuracy: 0.6347\n",
      "17/17 [==============================] - 0s 933us/step - loss: 0.2691 - accuracy: 0.6205\n",
      "Test Loss: 0.2691027820110321, Test Accuracy: 0.6204933524131775\n",
      "17/17 [==============================] - 0s 998us/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "max_len = 50  # Adjust as needed\n",
    "embedding_dim = 50  # Adjust as needed\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data['text'])\n",
    "X_train_cnn = pad_sequences(tokenizer.texts_to_sequences(train_data['text']), maxlen=max_len, padding='post')\n",
    "X_test_cnn = pad_sequences(tokenizer.texts_to_sequences(test_data['text']), maxlen=max_len, padding='post')\n",
    "\n",
    "# Labels\n",
    "y_train_cnn = train_data['label']\n",
    "y_test_cnn = test_data['label']\n",
    "\n",
    "# CNN model\n",
    "cnn_model = Sequential()\n",
    "cnn_model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim, input_length=max_len))\n",
    "cnn_model.add(Conv1D(128, 5, activation='relu'))\n",
    "cnn_model.add(GlobalMaxPooling1D())\n",
    "cnn_model.add(Dense(64, activation='relu'))\n",
    "cnn_model.add(Dropout(0.5))\n",
    "cnn_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "cnn_model.fit(X_train_cnn, y_train_cnn, epochs=5, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate on test set\n",
    "loss, accuracy = cnn_model.evaluate(X_test_cnn, y_test_cnn)\n",
    "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n",
    "\n",
    "# Make predictions\n",
    "cnn_predictions = cnn_model.predict(X_test_cnn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1a2bf4f2-4b1f-4e02-8035-eaa9f3af61ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "214/214 [==============================] - 1s 5ms/step - loss: 0.6192 - accuracy: 0.6984 - val_loss: 0.5908 - val_accuracy: 0.7253\n",
      "Epoch 2/5\n",
      "214/214 [==============================] - 1s 4ms/step - loss: 0.6131 - accuracy: 0.6991 - val_loss: 0.5882 - val_accuracy: 0.7253\n",
      "Epoch 3/5\n",
      "214/214 [==============================] - 1s 4ms/step - loss: 0.6133 - accuracy: 0.6991 - val_loss: 0.5898 - val_accuracy: 0.7253\n",
      "Epoch 4/5\n",
      "214/214 [==============================] - 1s 5ms/step - loss: 0.6127 - accuracy: 0.6991 - val_loss: 0.5911 - val_accuracy: 0.7253\n",
      "Epoch 5/5\n",
      "214/214 [==============================] - 1s 5ms/step - loss: 0.6127 - accuracy: 0.6991 - val_loss: 0.5900 - val_accuracy: 0.7253\n",
      "17/17 [==============================] - 0s 1ms/step - loss: 0.6170 - accuracy: 0.6926\n",
      "RNN Test Loss: 0.6170258522033691, Test Accuracy: 0.6925995945930481\n",
      "17/17 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "max_len = 50  # Adjust as needed\n",
    "embedding_dim = 50  # Adjust as needed\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data['text'])\n",
    "X_train_rnn = pad_sequences(tokenizer.texts_to_sequences(train_data['text']), maxlen=max_len, padding='post')\n",
    "X_test_rnn = pad_sequences(tokenizer.texts_to_sequences(test_data['text']), maxlen=max_len, padding='post')\n",
    "\n",
    "# Labels\n",
    "y_train_rnn = train_data['label']\n",
    "y_test_rnn = test_data['label']\n",
    "\n",
    "# RNN model\n",
    "rnn_model = Sequential()\n",
    "rnn_model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim, input_length=max_len))\n",
    "rnn_model.add(SimpleRNN(64, activation='relu'))\n",
    "rnn_model.add(Dense(64, activation='relu'))\n",
    "rnn_model.add(Dropout(0.5))\n",
    "rnn_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# optimizer = Adam(learning_rate=0.001)\n",
    "# rnn_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "# optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.01)\n",
    "# optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "# loss = tf.keras.losses.LogCosh()\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "# rnn_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "rnn_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Train the model\n",
    "rnn_model.fit(X_train_rnn, y_train_rnn, epochs=5, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate on test set\n",
    "loss, accuracy = rnn_model.evaluate(X_test_rnn, y_test_rnn)\n",
    "print(f'RNN Test Loss: {loss}, Test Accuracy: {accuracy}')\n",
    "\n",
    "# Make predictions\n",
    "rnn_predictions = rnn_model.predict(X_test_rnn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cac386c0-284c-4348-9892-ee714a63feb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence: [[12, 216, 90, 38, 25]]\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Grammatically \u001b[1m CORRECT \u001b[0m with probability:  0.6955893\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def evaluate_sentence(sentence, model, tokenizer, max_len):\n",
    "    # Tokenize and pad the input sentence\n",
    "    sequence = tokenizer.texts_to_sequences([sentence])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_len, padding='post')\n",
    "\n",
    "    # Print for debugging\n",
    "    print(f'Sequence: {sequence}')\n",
    "    # print(f'Padded Sequence: {padded_sequence}')\n",
    "\n",
    "    # Make prediction using the trained model\n",
    "    likelihood = model.predict(padded_sequence)[0][0]\n",
    "\n",
    "    return likelihood\n",
    "\n",
    "# Example usage:\n",
    "sentence_to_evaluate = \"He come no her more.\"\n",
    "result = evaluate_sentence(sentence_to_evaluate, rnn_model, tokenizer, max_len)\n",
    "if result<0.6:\n",
    "    print(\"Grammatically \\033[1m INCORRECT \\033[0m with probability: \",1- result)\n",
    "else:\n",
    "    print(\"Grammatically \\033[1m CORRECT \\033[0m with probability: \",result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c253a126-59e0-4334-82a7-dcbf11bb6bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7240997"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.7240997"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b41e9d-41fe-4e13-86ba-91a91881178f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785524df-365e-4eee-97f6-529c7a5ab4bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89f4b6f-8124-43d9-89d3-244a5e2a4b9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe98373b-edc0-49bc-84bc-3561309b4584",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bdfd0caa-7aa7-4639-8d0c-9d0f23167b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 1ms/step\n",
      "17/17 [==============================] - 0s 971us/step\n",
      "Stacking Ensemble Accuracy: 0.5660377358490566\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout, Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "# Make predictions on the test set\n",
    "rnn_predictions = rnn_model.predict(X_test_rnn)\n",
    "cnn_predictions = cnn_model.predict(X_test_cnn)\n",
    "\n",
    "# Create a new dataset with predictions from both models as features\n",
    "stacking_dataset = np.column_stack((rnn_predictions, cnn_predictions))\n",
    "\n",
    "# Split the stacking dataset for training the meta-model\n",
    "stacking_train, stacking_val, y_train_stacking, y_val_stacking = train_test_split(\n",
    "    stacking_dataset, y_test_rnn, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train a randomforest classification meta-model on the stacking dataset\n",
    "meta_model = RandomForestClassifier()\n",
    "meta_model.fit(stacking_train, y_train_stacking)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "meta_predictions = meta_model.predict(stacking_val)\n",
    "\n",
    "# Evaluate the stacking ensemble on the validation set\n",
    "ensemble_accuracy = accuracy_score(y_val_stacking, meta_predictions)\n",
    "print(f'Stacking Ensemble Accuracy: {ensemble_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1587d2f8-2207-4dc6-a089-1fe89a0eaada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,\n",
       "       1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a7d967f2-7e1f-4c94-82a4-20c6da01f6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268/268 [==============================] - 0s 981us/step\n",
      "268/268 [==============================] - 0s 796us/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Grader:\n",
    "    def __init__(self, rnn_model, cnn_model, tokenizer):\n",
    "        self.rnn_model = rnn_model\n",
    "        self.cnn_model = cnn_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.meta_model = None  # Initialize meta_model to None\n",
    "\n",
    "    def evaluate(self, text):\n",
    "        # Preprocess the input text for RNN\n",
    "        rnn_encoded_text = pad_sequences(self.tokenizer.texts_to_sequences([text]), maxlen=max_len, padding='post')\n",
    "        rnn_prediction = self.rnn_model.predict(rnn_encoded_text)[0].squeeze()\n",
    "\n",
    "        # Preprocess the input text for CNN\n",
    "        cnn_encoded_text = self.tokenizer.texts_to_sequences([text])\n",
    "        cnn_encoded_text = pad_sequences(cnn_encoded_text, maxlen=max_len, padding='post')\n",
    "        cnn_prediction = self.cnn_model.predict(cnn_encoded_text)[0].squeeze()\n",
    "\n",
    "        # Create a stacking dataset with predictions from both models as features\n",
    "        stacking_input = np.array([[rnn_prediction, cnn_prediction]])\n",
    "\n",
    "        if self.meta_model is not None:\n",
    "            # Make a prediction with the logistic regression meta-model\n",
    "            meta_prediction = self.meta_model.predict(stacking_input)\n",
    "            return meta_prediction.item()\n",
    "        else:\n",
    "            print(\"Meta-model not trained. Call train_meta_model before evaluate.\")\n",
    "            return None\n",
    "\n",
    "    def train_meta_model(self, X_train_rnn, X_train_cnn, y_train_cnn):\n",
    "        # Make predictions on the training set\n",
    "        rnn_predictions = self.rnn_model.predict(X_train_rnn)\n",
    "        cnn_predictions = self.cnn_model.predict(X_train_cnn)\n",
    "\n",
    "        # Create a stacking dataset with predictions from both models as features\n",
    "        stacking_dataset = np.column_stack((rnn_predictions, cnn_predictions))\n",
    "\n",
    "        # Train a logistic regression meta-model on the stacking dataset\n",
    "        self.meta_model = RandomForestClassifier()\n",
    "        self.meta_model.fit(stacking_dataset, y_train_cnn)\n",
    "\n",
    "\n",
    "# Create an instance of the Grader class with the trained models and tokenizer\n",
    "mygrader = Grader(rnn_model, cnn_model, tokenizer)\n",
    "\n",
    "# Train the meta-model\n",
    "mygrader.train_meta_model(X_train_rnn, X_train_cnn, y_train_cnn)\n",
    "\n",
    "# Example usage\n",
    "output = mygrader.evaluate(\"He donâ€™t come here no more.\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6831131d-bd79-4841-b611-d84939147307",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1af7ff38-d2cc-4586-b4be-24d0134be2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance on the test set\n",
    "test_predictions = meta_model.predict(stacking_val)\n",
    "# Compute precision and recall using test_predictions and test_labels\n",
    "# You may need to threshold the predictions based on your specific needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dc3b010e-dd1b-412a-b4a1-0370a0e8b5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.631578947368421\n",
      "Recall: 0.7272727272727273\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "precision = precision_score(y_val_stacking, test_predictions)\n",
    "recall = recall_score(y_val_stacking, test_predictions)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909413d1-40f8-4977-bc5c-4703b920cf6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7e638c-934c-4445-94f8-46925597e75e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b90d75f-f798-4fdf-b48d-2de15ba793fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
